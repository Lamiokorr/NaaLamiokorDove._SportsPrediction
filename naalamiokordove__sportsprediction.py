# -*- coding: utf-8 -*-
"""NaaLamiokorDove._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rnIeyjLDHu7ASnvdrpqLg96Jcr4MZqYg
"""

# Importing of necessary Python libraries and features to be used in the code
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from google.colab import drive
drive.mount('/content/drive')

"""**Loading of the datasets to be used for training**

```

```


"""

training_data = pd.read_csv("/content/drive/MyDrive/male_players (legacy).csv" ,index_col = 0)

"""**This gives a brief description of what the data entails**"""

training_data.info()

"""**Dropping unnecessary columns**"""

columns_to_drop = ['player_url','player_face_url','fifa_update_date','fifa_version']
training_data = training_data.drop(columns_to_drop, axis=1 )

""":**Displaying the numerical and categorical features**"""

num_cols = training_data.select_dtypes(include=['int64', 'float64']).columns
cat_cols = training_data.select_dtypes(include=['object']).columns

"""**Dropping columns with more than 30% threshold missing values in the training dataset**"""

missing_percentage_train = training_data.isnull().sum() / len(training_data) * 100
columns_to_drop_training_data = missing_percentage_train[missing_percentage_train > 30].index
training_data.drop(columns_to_drop_training_data, axis=1, inplace=True)

columns_to_drop_training_data

training_data.info()

"""**Separating training data into numeric and categorical features**

---


"""

numeric_features_training_data = training_data.select_dtypes(include=['number'])
categorical_features_training_data = training_data.select_dtypes(exclude=['number'])

numeric_features_training_data.info()

categorical_features_training_data.info()

"""**Using SimpleImputer to impute missing values for both numeric and categorical features in the training data**"""

numeric_imputer = SimpleImputer(strategy='mean')
training_data[numeric_features_training_data.columns] = numeric_imputer.fit_transform(training_data[numeric_features_training_data.columns])

categorical_imputer = SimpleImputer(strategy='most_frequent')
training_data[categorical_features_training_data.columns] = categorical_imputer.fit_transform(training_data[categorical_features_training_data.columns])

"""**Encoding the categorical features in the columns with LabelEncoder**"""

label_encoder = LabelEncoder()
for column in categorical_features_training_data.columns:
    training_data[column] = training_data[column].astype(str)
    training_data[column] = label_encoder.fit_transform(training_data[column])

"""**Seperating the data into features(X) and target variable(y)**



"""

X = training_data.drop(columns=['overall'])
y = training_data['overall']

"""**Performing correlation to check the features with high correlation to the target variable**"""

corr = X.corrwith(y)
sorted_corr = corr.abs().sort_values(ascending=False)
corr_list = list(zip(sorted_corr.index, sorted_corr.tolist()))

corr_df = pd.DataFrame(corr_list, columns=['Feature', 'Correlation'])

top_features = corr_df.head(7)['Feature'].tolist()
X_subset = X[top_features]
X_subset

"""**Scaling the data**"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_subset_scaled = scaler.fit_transform(X_subset)

"""**Dividing the data into training and testing sets for cross-validation**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Performing hyperparameter tuning/ evaluation of XGBoost Regressor model using Cross-Validation**"""

from xgboost import XGBRegressor
xgb_model = XGBRegressor(random_state=42)

#Specifying the number of splits for the K-fold cross-validation
no_of_splits = 3

params = {'n_estimators': [10, 20, 30],'learning_rate': [0.1, 0.01, 0.001]}
xgb_grid = GridSearchCV(xgb_model, params, cv = no_of_splits, scoring='neg_mean_absolute_error', n_jobs=-1)

xgb_grid.fit(X_train, y_train)
xgb_model = xgb_grid.best_estimator_

y_pred = xgb_grid.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print('Cross-Validation Mean Squared Error Scores for XGBoost:', mse)
print('Cross-Validation Mean Absolute Error Scores for XGBoost:', mae)
print('Cross-Validation R-squared Scores for XGBoost:', r2)

"""**Performing Hyperparameter tuning/ evaluation of RandomForestRegressor model using Cross-Validation** **bold text**"""

from sklearn.ensemble import RandomForestRegressor
rfr_model = RandomForestRegressor(random_state=42)

#Specifying the number of splits for the K-fold cross validation
no_of_splits = 3


params = {'n_estimators': [10, 20, 30],'max_depth':[10,30,40]}
rfr_grid = GridSearchCV(rfr_model, params, cv = no_of_splits, scoring='neg_mean_absolute_error', n_jobs=-1)

rfr_grid.fit(X_train, y_train)
rfr_model = rfr_grid.best_estimator_

y_pred = rfr_grid.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print('Cross-Validation Mean Squared Error Scores for RandomForestRegressor:', mse)
print('Cross-Validation Mean Absolute Error Scores for RandomForestRegressor:', mae)
print('Cross-Validation R-squared Scores for RandomForestRegressor:', r2)

from sklearn.ensemble import GradientBoostingRegressor
gbr_model = GradientBoostingRegressor(random_state=42)

#Specifying the number of splits for the K-fold cross validation
no_of_splits = 3


params = {'n_estimators': [10, 20, 30],'learning_rate': [0.1, 0.01, 0.001]}
gbr_grid = GridSearchCV(gbr_model, params, cv = no_of_splits, scoring='neg_mean_absolute_error', n_jobs=-1)

gbr_grid.fit(X_train, y_train)
gbr_model = gbr_grid.best_estimator_

y_pred = gbr_grid.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print('Cross-Validation Mean Squared Error Scores for GradientBoostRegressor:', mse)
print('Cross-Validation Mean Absolute Error Scores for GradientBoostRegressor:', mae)
print('Cross-Validation R-squared Scores for GradientBoostRegressor:', r2)

import joblib

#Saving the trained model
joblib.dump(rfr_model, 'Player Ratings.pkl')

#Saving the scaler
joblib.dump(scaler, 'scaler.pkl')

"""**Loading of the players_22-1 dataset**"""

testing_data = pd.read_csv("/content/drive/MyDrive/players_22-1.csv",)

testing_data.info()

"""**Dropping unneccessary columns**"""

columns_to_drop = ['player_url','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url']
testing_data = testing_data.drop(columns_to_drop, axis=1 )

#Dropping columns with more than 30% of their data missing
missing_percentage_test = testing_data.isnull().sum() / len(testing_data) * 100
columns_to_drop_testing_data = missing_percentage_test[missing_percentage_test > 30].index
testing_data.drop(columns_to_drop_testing_data, axis=1, inplace=True)

columns_to_drop_testing_data

"""**Separating the testing dataset into numeric and categorical features**

---


"""

numeric_features_testing_data = testing_data.select_dtypes(include=['number'])
categorical_features_testing_data = testing_data.select_dtypes(exclude=['number'])

"""**Imputing the missing features of the testing dataset.**"""

numeric_imputer = SimpleImputer(strategy='mean')
testing_data[numeric_features_testing_data.columns] = numeric_imputer.fit_transform(testing_data[numeric_features_testing_data.columns])

categorical_imputer = SimpleImputer(strategy='most_frequent')
testing_data[categorical_features_testing_data.columns] = categorical_imputer.fit_transform(testing_data[categorical_features_testing_data.columns])

#testing_data[categorical_features_testing_data.columns] = pd.DataFrame(training_data[categorical_features_training_data.columns], columns=training_data.columns)
#testing_data[categorical_features_testing_data.columns] = pd.DataFrame(training_data[categorical_features_training_data.columns],
                                                                     # columns=categorical_features_testing_data.columns)

testing_data = testing_data.reset_index(drop=True)
training_data = training_data.reset_index(drop=True)

# Now perform the column replacement
testing_data[categorical_features_testing_data.columns] = pd.DataFrame(training_data[categorical_features_training_data.columns],
                                                                      columns=categorical_features_testing_data.columns)

"""**Encoding the Categorical Features with LabelEncoder**"""

label_encoder = LabelEncoder()
for column in categorical_features_testing_data.columns:
    testing_data[column] = testing_data[column].astype(str)
    testing_data[column] = label_encoder.fit_transform(testing_data[column])

X_players22 = testing_data.drop(columns=['overall'])
y_players22 = testing_data['overall']

X_players22.info()

fitted_feature_names = rfr_grid.feature_names_in_

# Check the current feature names in X_players22
current_feature_names = X_players22.columns

# Rename columns in X_players22 if necessary
X_players22 = X_players22.rename(columns={
    'sofifa_id': 'league_id',
    'club_joined': 'club_joined_date',
    'club_contract_valid_until': 'club_contract_valid_until_year',
    'release_clause_eur': 'fifa_update'
})

# Add missing columns with default values (e.g., NaN or a suitable default)
for feature in fitted_feature_names:
    if feature not in X_players22.columns:
        X_players22[feature] = pd.NA  # or an appropriate default value

# Reorder columns to match the fitted feature names
X_players22 = X_players22[fitted_feature_names]

# Ensure there are no extra columns
X_players22 = X_players22.loc[:, fitted_feature_names]

# Now, predict
y_pred_players22 = rfr_grid.predict(X_players22)

"""***The RandomForestRegressor model was chosen as the best model since compared to the other models, it had the lowest Mean Absolute Score. ***

**Saving the y_test and y_pred values in a CSV file**
"""

data = {'y_test': y_players22, 'y_pred': y_pred_players22}
new_df = pd.DataFrame(data)
new_df.to_csv('y_test_and_y_pred.csv', index=False)

"""**Evaluating the model's performance on the player_22 dataset with metrics such as MSE, MAE and R squared scores**"""

mse_players22 = mean_squared_error(y_players22, y_pred_players22)
mae_players22 = mean_absolute_error(y_players22, y_pred_players22)
r2_players22 = r2_score(y_players22, y_pred_players22)
print('Mean Squared Error for Players_22 dataset:', mse_players22)
print('Mean Absolute Error for Players_22 dataset:', mae_players22)
print('R-squared score for Players_22 dataset:', r2_players22)

